{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural closure models for the viscous Burgers equation\n",
    "\n",
    "In this tutorial, we will train neural closure models for the viscous Burgers\n",
    "equation in the [Julia](https://julialang.org/) programming language. We here\n",
    "use Julia for ease of use, efficiency, and writing differentiable code to\n",
    "power scientific machine learning. This file is available as\n",
    "\n",
    "- a commented [Julia script](https://github.com/agdestein/NeuralClosure/blob/main/burgers.jl),\n",
    "- a [markdown file](https://github.com/agdestein/NeuralClosure/blob/main/generated/burgers.md),\n",
    "- a Jupyter [notebook](https://github.com/agdestein/NeuralClosure/blob/main/generated/burgers.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running this notebook on Google Colab\n",
    "\n",
    "_This section is only needed when running on Google colab._\n",
    "_If you run this notebook on your local machine, skip this section._\n",
    "\n",
    "To use Julia on Google colab, we will install Julia using the official version\n",
    "manager Juliup. From the default Python runtime, we can access the shell by\n",
    "starting a line with `!`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "!curl -fsSL https://install.julialang.org | sh -s -- --yes"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check that Julia is successfully installed on the Colab instance."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "!~/.juliaup/bin/julia -e 'println(\"Hello\")'"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now proceed to install the necessary Julia packages, including `IJulia` which\n",
    "will add the Julia notebook kernel."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "%%shell\n",
    "~/.juliaup/bin/julia -e '''\n",
    "    using Pkg\n",
    "    Pkg.add([\n",
    "        \"ComponentArrays\",\n",
    "        \"FFTW\",\n",
    "        \"IJulia\",\n",
    "        \"LinearAlgebra\",\n",
    "        \"Lux\",\n",
    "        \"NNlib\",\n",
    "        \"Optimisers\",\n",
    "        \"Plots\",\n",
    "        \"Printf\",\n",
    "        \"Random\",\n",
    "        \"SparseArrays\",\n",
    "        \"Zygote\",\n",
    "    ])\n",
    "'''"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once this cell has finished running (this may take a few minutes,\n",
    "depending on what resources Colab decides to give you), do the following:\n",
    "\n",
    "1. Reload the browser page (`CTRL`/`CMD` + `R`)\n",
    "2. In the top right corner of Colab, then select the Julia kernel.\n",
    "   ![](https://github.com/agdestein/NeuralClosure/blob/main/assets/select.png?raw=true)\n",
    "   ![](https://github.com/agdestein/NeuralClosure/blob/main/assets/runtime.png?raw=true)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the simulations\n",
    "\n",
    "Julia comes with many built in features, including array functionality.\n",
    "Additional functionality is provided in various packages, some of which are\n",
    "available in the Standard Library (LinearAlgebra, Printf, Random,\n",
    "SparseArrays). Others are available in the General Registry, and can be added\n",
    "using the built in package manager Pkg, e.g. `using Pkg; Pkg.add(\"Plots\")`.\n",
    "If you ran the Colab setup section, the packages should already be added.\n",
    "\n",
    "If you cloned the NeuralClosure repository and run this in VSCode,\n",
    "there should be a file called `Project.toml`, specifying dependencies.\n",
    "This file specifies an environment, which can be activated.\n",
    "You can then install the dependencies by uncommenting and running the\n",
    "following cell:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# using Pkg\n",
    "# Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alernatively, you can add them manually to your global environment:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# using Pkg\n",
    "# Pkg.add([\n",
    "#     \"ComponentArrays\",\n",
    "#     \"FFTW\",\n",
    "#     \"LinearAlgebra\",\n",
    "#     \"Lux\",\n",
    "#     \"NNlib\",\n",
    "#     \"Optimisers\",\n",
    "#     \"Plots\",\n",
    "#     \"Printf\",\n",
    "#     \"Random\",\n",
    "#     \"SparseArrays\",\n",
    "#     \"Zygote\",\n",
    "# ])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ComponentArrays\n",
    "using FFTW\n",
    "using LinearAlgebra\n",
    "using Lux\n",
    "using NNlib\n",
    "using Optimisers\n",
    "using Plots\n",
    "using Printf\n",
    "using Random\n",
    "using SparseArrays\n",
    "using Zygote"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we have loaded the reverse mode AD framework\n",
    "[Zygote](https://github.com/FluxML/Zygote.jl). This package provides the\n",
    "function `gradient`, which is able to differentiate functions that we write,\n",
    "including our numerical solvers (to be defined in the next section).\n",
    "Gradients allow us to perform gradient descent, and thus \"train\" our neural\n",
    "networks.\n",
    "\n",
    "The deep learning framework [Lux](https://lux.csail.mit.edu/) likes to toss\n",
    "random number generators (RNGs) around, for reproducible science. We\n",
    "therefore need to initialize an RNG. The seed makes sure the same sequence of\n",
    "pseudo-random numbers are generated at each time the session is restarted."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(123)\n",
    "rng = Random.default_rng()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deep learning functions usually use single precision floating point numbers\n",
    "by default, as this is preferred on GPUs. Julia itself, on the other hand, is\n",
    "slightly opinionated towards double precision floating point numbers, e.g.\n",
    "`3.14`, `1 / 6` and `2π` are all of type `Float64` (their single precision\n",
    "counterparts would be the slightly more verbose `3.14f0`, `1f0 / 6` (or\n",
    "`Float32(1 / 6)`) and `2f0π`). For simplicity, we will only use `Float64`.\n",
    "The only place we will encounter `Float32` in this file is in the default\n",
    "neural network weight initializers, so here is an alternative weight\n",
    "initializer using double precision."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "glorot_uniform_64(rng::AbstractRNG, dims...) = glorot_uniform(rng, Float64, dims...)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The viscous Burgers equation\n",
    "\n",
    "Consider the periodic domain $\\Omega = [0, 1]$. The viscous Burgers equation\n",
    "is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = - \\frac{1}{2} \\frac{\\partial }{\\partial x}\n",
    "\\left( u^2 \\right) + \\nu \\frac{\\partial^2 u}{\\partial x^2},\n",
    "$$\n",
    "\n",
    "where $\\nu > 0$ is the viscosity. The Burgers equation models the velocity\n",
    "profile of a compressible fluid, and has the particularity of creating\n",
    "shocks, which are dampened by the viscosity.\n",
    "\n",
    "### Discretization\n",
    "\n",
    "For simplicity, we will use a uniform discretization $x = \\left( \\frac{n}{N}\n",
    "\\right)_{n = 1}^N$, with the additional point $x_0 = 0$ overlapping with\n",
    "$x_N$. The step size is $\\Delta x = \\frac{1}{N}$. Using a central finite\n",
    "difference, we get the discrete equations\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} u_n}{\\mathrm{d} t} = - \\frac{1}{2} \\frac{u_{n + 1}^2 - u_{n -\n",
    "1}^2}{2 \\Delta x} + \\nu \\frac{u_{n + 1} - 2 u_n + u_{n - 1}}{\\Delta x^2},\n",
    "$$\n",
    "\n",
    "with the convention $u_0 = u_N$ and $u_{N + 1} = u_1$ (periodic extension). The\n",
    "degrees of freedom are stored in the vector $u = (u_n)_{n = 1}^N$. In vector\n",
    "notation, we will write this as\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} u}{\\mathrm{d} t} = f(u).\n",
    "$$\n",
    "\n",
    "Solving this equation for sufficiently small $\\Delta x$ (sufficiently large\n",
    "$N$) will be referred to as _direct numerical simulation_ (DNS), and can be\n",
    "expensive.\n",
    "\n",
    "Note: This is a simple discretization, not ideal for dealing with shocks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by defining the right hand side function `f` for a vector `u`, making\n",
    "sure to account for the periodic boundaries. The macro `@.` makes sure that all\n",
    "following operations are performed element-wise. Note that `circshift` here\n",
    "acts along the first dimension, so `f` can be applied to multiple snapshots\n",
    "at once (stored as columns in the matrix `u`). Note the semicolon `;` in the\n",
    "function signature: It is used to separate positional arguments (here `u`)\n",
    "from keyword arguments (here `ν`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function f(u; ν)\n",
    "    N = size(u, 1)\n",
    "    u₊ = circshift(u, -1)\n",
    "    u₋ = circshift(u, 1)\n",
    "    @. -N / 4 * (u₊^2 - u₋^2) + N^2 * ν * (u₊ - 2u + u₋)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time discretization\n",
    "\n",
    "For time stepping, we do a simple explicit Runge-Kutta scheme (RK).\n",
    "\n",
    "From a current state $u^0 = u(t)$, we divide the outer time step\n",
    "$\\Delta t$ into $i \\in \\{1, \\dots, s\\}$ sub-steps as follows:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "f^i & = f(u^{i - 1}) \\\\\n",
    "u^i & = u^0 + \\Delta t \\sum_{j = 1}^{i} A_{i j} f^j,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{s \\times s}$ are the coefficients of the RK method.\n",
    "The solution at the next outer time step $t + \\Delta t$ is then $u^s = u(t +\n",
    "\\Delta t) + \\mathcal{O}(\\Delta t^{r + 1})$ if we start exactly from $u(t)$,\n",
    "where $r$ is the order of the RK method. If we chain multiple steps from the\n",
    "initial conditions $u(0)$ to a final state $u(t)$, the total error is of\n",
    "order $\\mathcal{O}(\\Delta t^r)$.\n",
    "\n",
    "A fourth order method is given by the following coefficients ($s = 4$, $r =\n",
    "4$):\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 & 0 \\\\\n",
    "    \\frac{1}{6} & \\frac{2}{6} & \\frac{2}{6} & \\frac{1}{6}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The following function performs one RK4 time step. Note that we never modify\n",
    "any vectors, only create new ones. The AD-framework Zygote prefers it this\n",
    "way. The splatting syntax `params...` lets us pass a variable number of\n",
    "keyword arguments to the right hand side function `f` (for the above `f`\n",
    "there is only one: `ν`). Similarly, `k...` splats the tuple `k`, but but now\n",
    "like a positional arguement instead of keyword arguments (without names)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function step_rk4(f, u₀, dt; params...)\n",
    "    A = [\n",
    "        1 0 0 0\n",
    "        0 1 0 0\n",
    "        0 0 1 0\n",
    "        1/6 2/6 2/6 1/6\n",
    "    ]\n",
    "    u = u₀\n",
    "    k = ()\n",
    "    for i = 1:size(A, 1)\n",
    "        ki = f(u; params...)\n",
    "        k = (k..., ki)\n",
    "        u = u₀\n",
    "        for j = 1:i\n",
    "            u = u + dt * A[i, j] * k[j]\n",
    "        end\n",
    "    end\n",
    "    u\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solving the ODE is done by chaining individual time steps. We here add the\n",
    "option to call a callback function after each time step. Note that the path\n",
    "to the final output `u` is obtained by passing the inputs `u₀` and\n",
    "parameters `params` through a finite amount of computational steps, each of\n",
    "which should have a chain rule defined and recognized in the Zygote AD\n",
    "framework. The the ODE solution `u` should be differentiable (with respect to\n",
    "`u₀` or `params`), as long as `f` is."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function solve_ode(f, u₀; dt, nt, callback = (u, t, i) -> nothing, ncallback = 1, params...)\n",
    "    t = 0.0\n",
    "    u = u₀\n",
    "    for i = 1:nt\n",
    "        t += dt\n",
    "        u = step_rk4(f, u, dt; params...)\n",
    "        if i % ncallback == 0\n",
    "            callback(u, t, i)\n",
    "        end\n",
    "    end\n",
    "    u\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the initial conditions, we create a random spectrum with a spectral\n",
    "amplitude decay profile (default: $\\frac{1}{(1 + | k |)^{6/5}}$)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_initial_conditions(\n",
    "    nx,\n",
    "    nsample;\n",
    "    kmax = 10,\n",
    "    decay = k -> 1 / (1 + abs(k))^1.2,\n",
    ")\n",
    "    # Fourier basis\n",
    "    basis = [exp(2π * im * k * x / nx) for x ∈ 1:nx, k ∈ -kmax:kmax]\n",
    "\n",
    "    # Fourier coefficients with random phase and amplitude\n",
    "    c = [randn() * exp(-2π * im * rand()) * decay(k) for k ∈ -kmax:kmax, _ ∈ 1:nsample]\n",
    "\n",
    "    # Random data samples (real-valued)\n",
    "    real.(basis * c)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example simulation\n",
    "\n",
    "Let's test our method in action."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ν = 1.0e-3\n",
    "nx = 512\n",
    "x = LinRange(0.0, 1.0, nx + 1)[2:end]\n",
    "\n",
    "# Initial conditions (one sample vector)\n",
    "u = create_initial_conditions(nx, 1)\n",
    "\n",
    "# Time stepping\n",
    "u = solve_ode(\n",
    "    f,\n",
    "    u;\n",
    "    dt = 1.0e-3,\n",
    "    nt = 2000,\n",
    "    ncallback = 20,\n",
    "    callback = (u, t, i) -> begin\n",
    "        title = @sprintf(\"Solution, t = %.3f\", t)\n",
    "        fig = plot(x, u; xlabel = \"x\", title)\n",
    "        display(fig)\n",
    "        sleep(0.01) # Time for plot to render\n",
    "    end,\n",
    "    ν,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is typical for the Burgers equation: The initial conditions merge to\n",
    "form a shock, which is eventually dampened due to the viscosity. If we let\n",
    "the simulation go on, diffusion will take over and we get a smooth solution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discrete filtering and large eddy simulation (LES)\n",
    "\n",
    "We now assume that we are only interested in the large scale structures of the\n",
    "flow. To compute those, we would ideally like to use a coarser resolution\n",
    "($N_\\text{LES}$) than the one needed to resolve all the features of the flow\n",
    "($N_\\text{DNS}$).\n",
    "\n",
    "To define \"large scales\", we consider a discrete filter $\\phi \\in\n",
    "\\mathbb{R}^{N_\\text{LES} \\times N_\\text{DNS}}$, averaging multiple DNS grid\n",
    "points into LES points. The filtered velocity field is defined by $\\bar{u} =\n",
    "\\phi u$. It is governed by the equation\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\bar{u}}{\\mathrm{d} t} = f(\\bar{u}) + c(u, \\bar{u}),\n",
    "$$\n",
    "\n",
    "where $f$ is adapted to the grid of its input field ($\\bar{u}$) and\n",
    "$c(u, \\bar{u}) = \\overline{f(u)} - f(\\bar{u})$ is the commutator error\n",
    "between the coarse grid and filtered fine grid right hand sides. To close the\n",
    "equations, we approximate the unknown commutator error using a closure model\n",
    "$m$ with parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "m(\\bar{u}, \\theta) \\approx c(u, \\bar{u}).\n",
    "$$\n",
    "\n",
    "We thus need to make two choices: $m$ and $\\theta$. While the model $m$ will\n",
    "be chosen based on our expertise of numerical methods and machine learning,\n",
    "the parameters $\\theta$ will be fitted using high fidelity simulation data.\n",
    "We can then solve the LES equation\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} \\bar{v}}{\\mathrm{d} t} = f(\\bar{v}) + m(\\bar{v}, θ),\n",
    "$$\n",
    "\n",
    "where the LES solution $\\bar{v}$ is an approximation to the filtered DNS\n",
    "solution $\\bar{u}$.\n",
    "\n",
    "The following right hand side function includes the correction term."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "g(u; ν, m, θ) = f(u; ν) + m(u, θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data generation\n",
    "\n",
    "This generic function creates a data structure containing filtered DNS data,\n",
    "commutator errors and simulation parameters for a given filter $ϕ$. We also\n",
    "provide some default values."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_data(\n",
    "    nsample,\n",
    "    ϕ;\n",
    "    dt = 1.0e-3,\n",
    "    nt = 1000,\n",
    "    kmax = 10,\n",
    "    decay = k -> 1 / (1 + abs(k))^1.2,\n",
    "    ν = 1.0e-3,\n",
    ")\n",
    "    # Resolution\n",
    "    nx_les, nx_dns = size(ϕ)\n",
    "\n",
    "    # Grids\n",
    "    x_les = LinRange(0.0, 1.0, nx_les + 1)[2:end]\n",
    "    x_dns = LinRange(0.0, 1.0, nx_dns + 1)[2:end]\n",
    "\n",
    "    # Output data\n",
    "    data = (;\n",
    "        # Filtered snapshots and commutator errors (including at t = 0)\n",
    "        u = zeros(nx_les, nsample, nt + 1),\n",
    "        c = zeros(nx_les, nsample, nt + 1),\n",
    "\n",
    "        # Simulation-specific parameters\n",
    "        dt,\n",
    "        ν,\n",
    "    )\n",
    "\n",
    "    # DNS Initial conditions\n",
    "    u = create_initial_conditions(nx_dns, nsample; kmax, decay)\n",
    "\n",
    "    # Save filtered solution and commutator error after each DNS time step\n",
    "    function callback(u, t, i)\n",
    "        ubar = ϕ * u\n",
    "        data.u[:, :, i+1] = ubar\n",
    "        data.c[:, :, i+1] = ϕ * f(u; ν) - f(ubar; ν)\n",
    "        if i % 10 == 0\n",
    "            title = @sprintf(\"Solution, t = %.3f\", t)\n",
    "            np = min(3, nsample)\n",
    "            fig = plot(; xlabel = \"x\", title)\n",
    "            plot!(fig, x_dns, u[:, 1:np]; color = (1:np)', linestyle = :dash, label = \"u\")\n",
    "            plot!(fig, x_les, ubar[:, 1:np]; color = (1:np)', label = \"ubar\")\n",
    "            display(fig)\n",
    "            sleep(0.001) # Time for plot to render\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Save for t = 0.0 first\n",
    "    callback(u, 0.0, 0)\n",
    "\n",
    "    # Do time stepping (save after each step)\n",
    "    solve_ode(f, u; dt, nt, callback, ncallback = 1, ν)\n",
    "\n",
    "    # Return data\n",
    "    data\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choosing model parameters\n",
    "\n",
    "To choose $\\theta$, we will minimize a loss function using an gradient\n",
    "descent based optimization method (\"train\" the neural network)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss function\n",
    "\n",
    "Since the model is used to predict the commutator error, the obvious choice\n",
    "of loss function is the a priori loss function\n",
    "\n",
    "$$\n",
    "L^\\text{prior}(\\theta) = \\| m(\\bar{u}, \\theta) - c(u, \\bar{u}) \\|^2.\n",
    "$$\n",
    "\n",
    "This loss function has a simple computational chain, that is mostly comprised\n",
    "of evaluating the neural network $m$ itself. Computing the gradient with\n",
    "respect to $\\theta$ is thus simple. The gradient is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} L^\\text{prior}}{\\mathrm{d} t}(\\theta) = 2 (m(\\bar{u},\n",
    "\\theta) - c(u, \\bar{u}))^\\mathsf{T}\n",
    "\\frac{\\partial m}{\\partial \\theta}(\\bar{u}, \\theta),\n",
    "$$\n",
    "\n",
    "but we don't need to specify any of that, Zygote figures it out just fine on\n",
    "its own.\n",
    "\n",
    "We call this loss function \"a priori\" since it only measures the error of the\n",
    "prediction itself, and not the effect this error has on the LES solution\n",
    "$\\bar{v}_{\\theta}$. Since instability in $\\bar{v}_{\\theta}$ is not directly\n",
    "detected in this loss function, we add a regularization term to penalize\n",
    "extremely large weights."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mean_squared_error(m, u, c, θ; λ) =\n",
    "    sum(abs2, m(u, θ) - c) / sum(abs2, c) + λ * sum(abs2, θ) / length(θ)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will only use a random subset `nuse` of all `nsample * nt`\n",
    "solution snapshots at each loss evaluation. This random sampling creates a\n",
    "random variable in the loss function, which becomes stochastic. Minimizing it\n",
    "using gradient descent is thus called _stochastic gradient descent_.\n",
    "The `Zygote.@ignore` macro just tells the AD engine Zygote not to complain\n",
    "about the random index selection."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_randloss_commutator(m, data; nuse = 20, λ = 1.0e-8)\n",
    "    (; u, c) = data\n",
    "    u = reshape(u, size(u, 1), :)\n",
    "    c = reshape(c, size(c, 1), :)\n",
    "    nsample = size(u, 2)\n",
    "    @assert nsample ≥ nuse\n",
    "    function randloss(θ)\n",
    "        i = Zygote.@ignore sort(shuffle(1:nsample)[1:nuse])\n",
    "        uuse = Zygote.@ignore u[:, i]\n",
    "        cuse = Zygote.@ignore c[:, i]\n",
    "        mean_squared_error(m, uuse, cuse, θ; λ)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ideally, we want the LES simulation to produce the filtered DNS velocity\n",
    "$\\bar{u}$. The a priori loss does not guarantee or enforce this.\n",
    "We can alternatively minimize the a posteriori loss function\n",
    "\n",
    "$$\n",
    "L^\\text{post}(\\theta) = \\| \\bar{v}_\\theta - \\bar{u} \\|^2,\n",
    "$$\n",
    "\n",
    "where $\\bar{v}_\\theta$ is the solution to the LES equation for the given\n",
    "parameters. This loss function contains more information about the effect of\n",
    "$\\theta$ than $L^\\text{prior}$. However, it has a significantly longer\n",
    "computational chain, as it includes time stepping in addition to the neural\n",
    "network evaluation itself. Computing the gradient with respect to $\\theta$ is\n",
    "more costly, and also requires an AD-friendly time stepping scheme (which we\n",
    "have already taken care of above). Note that it is also possible to compute\n",
    "the gradient of the time-continuous ODE instead of the time-discretized one\n",
    "as we do here. It involves solving an adjoint ODE backwards in time, which in\n",
    "turn has to be discretized. Our approach here is therefore called\n",
    "\"discretize-then-optimize\", while the adjoint ODE method is called\n",
    "\"optimize-then-discretize\". The [SciML](https://github.com/sciml) time\n",
    "steppers include both methods, as well as useful strategies for evaluating\n",
    "them efficiently.\n",
    "\n",
    "For the a posteriori loss function, we provide the right hand side function\n",
    "`model` (including closure), reference trajectories `u`, and model\n",
    "parameters. We compute the error between the predicted and reference trajectories\n",
    "at each time point."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function trajectory_loss(model, u; dt, params...)\n",
    "    nt = size(u, 3)\n",
    "    loss = 0.0\n",
    "    v = u[:, :, 1]\n",
    "    for i = 2:nt\n",
    "        v = step_rk4(model, v, dt; params...)\n",
    "        ui = u[:, :, i]\n",
    "        loss += sum(abs2, v - ui) / sum(abs2, ui)\n",
    "    end\n",
    "    loss / (nt - 1)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also make a non-squared variant for error analysis."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function trajectory_error(model, u; dt, params...)\n",
    "    nt = size(u, 3)\n",
    "    loss = 0.0\n",
    "    v = u[:, :, 1]\n",
    "    for i = 2:nt\n",
    "        v = step_rk4(model, v, dt; params...)\n",
    "        ui = u[:, :, i]\n",
    "        loss += norm(v - ui) / norm(ui)\n",
    "    end\n",
    "    loss / (nt - 1)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To limit the length of the computational chain, we only unroll `nunroll`\n",
    "time steps at each loss evaluation. The time step from which to unroll is\n",
    "chosen at random at each evaluation, as are the initial conditions (`nuse`).\n",
    "\n",
    "The non-trainable parameters (e.g. $\\nu$) are passed in `params`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_randloss_trajectory(model, data; nuse = 1, nunroll = 10, params...)\n",
    "    (; u, dt) = data\n",
    "    nsample = size(u, 2)\n",
    "    nt = size(ubar, 3)\n",
    "    @assert nt ≥ nunroll\n",
    "    @assert nsample ≥ nuse\n",
    "    function randloss(θ)\n",
    "        isample = Zygote.@ignore sort(shuffle(1:nsample)[1:nuse])\n",
    "        istart = Zygote.@ignore rand(1:nt-nunroll)\n",
    "        it = Zygote.@ignore istart:istart+nunroll\n",
    "        uuse = Zygote.@ignore u[:, isample, it]\n",
    "        trajectory_loss(model, uuse; dt, params..., θ)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "During training, we will monitor the error on the validation dataset with a\n",
    "callback. We will plot the history of the a priori and a posteriori errors."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Initial empty history\n",
    "initial_callbackstate() = (; ihist = Int[], ehist_prior = zeros(0), ehist_post = zeros(0))\n",
    "\n",
    "# Create callback for given model and dataset\n",
    "function create_callback(m, data)\n",
    "    (; u, c, dt, ν) = data\n",
    "    uu, cc = reshape(u, size(u, 1), :), reshape(c, size(c, 1), :)\n",
    "    e_post_ref = trajectory_error(f, u; dt, ν)\n",
    "    function callback(i, θ, state)\n",
    "        (; ihist, ehist_prior, ehist_post) = state\n",
    "        state = (;\n",
    "            ihist = vcat(ihist, i),\n",
    "            ehist_prior = vcat(ehist_prior, norm(m(uu, θ) - cc) / norm(cc)),\n",
    "            ehist_post = vcat(ehist_post, trajectory_error(g, u; dt, ν, m, θ)),\n",
    "        )\n",
    "        fig = plot(; yscale = :log10, xlabel = \"Iterations\", title = \"Relative error\")\n",
    "        hline!(fig, [1.0]; color = 1, linestyle = :dash, label = \"A priori: No model\")\n",
    "        plot!(fig, state.ihist, state.ehist_prior; color = 1, label = \"A priori: Model\")\n",
    "        hline!(\n",
    "            fig,\n",
    "            [e_post_ref];\n",
    "            color = 2,\n",
    "            linestyle = :dash,\n",
    "            label = \"A posteriori: No model\",\n",
    "        )\n",
    "        plot!(fig, state.ihist, state.ehist_post; color = 2, label = \"A posteriori: Model\")\n",
    "        display(fig)\n",
    "        println(\"Iteration $i\")\n",
    "        state\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For training, we have to initialize an optimizer and a callbackstate (lots of\n",
    "state initilization in this session)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "initial_trainstate(optimiser, θ) = (;\n",
    "    opt = Optimisers.setup(optimiser, θ),\n",
    "    θ,\n",
    "    callbackstate = initial_callbackstate(),\n",
    "    istart = 0,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "    train(; loss, opt, θ, istart, niter, ncallback, callback, callbackstate)\n",
    "\n",
    "Perform `niter` iterations of gradient descent on `loss(θ)` for given `optimiser`.\n",
    "Every `ncallback` iteration, call `callback(i, θ, callbackstate)` and update\n",
    "callback state, where `i` starts at `istart`.\n",
    "\n",
    "Return a named tuple `(; opt, θ, callbcackstate)` containing the updated\n",
    "optimizer state, parameters and callback state.\n",
    "\"\"\"\n",
    "function train(; loss, opt, θ, istart, niter, ncallback, callback, callbackstate)\n",
    "    for i = 1:niter\n",
    "        ∇ = first(gradient(loss, θ))\n",
    "        opt, θ = Optimisers.update(opt, θ, ∇)\n",
    "        i % ncallback == 0 && (callbackstate = callback(istart + i, θ, callbackstate))\n",
    "    end\n",
    "    istart += niter\n",
    "    (; opt, θ, callbackstate, istart)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model architecture\n",
    "\n",
    "We are free to choose the model architecture $m$. Here, we will consider two\n",
    "neural network architectures. The following wrapper returns the model and\n",
    "initial parameters for a Lux `Chain`. Note: If the chain includes\n",
    "state-dependent layers such as `Dropout` (which modify their RNGs at each\n",
    "evaluation), this wrapper should not be used."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function create_model(chain, rng)\n",
    "    # Create parameter vector and empty state\n",
    "    θ, state = Lux.setup(rng, chain)\n",
    "\n",
    "    # Convert nested named tuples of arrays to a ComponentArray,\n",
    "    # which behaves like a long vector\n",
    "    θ = ComponentArray(θ)\n",
    "\n",
    "    # Convenience wrapper for empty state in input and output\n",
    "    m(v, θ) = first(chain(v, θ, state))\n",
    "\n",
    "    # Return model and initial parameters\n",
    "    m, θ\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolutional neural network architecture (CNN)\n",
    "\n",
    "A CNN is an interesting closure model for the following reasons:\n",
    "\n",
    "- The parameters are sparse, since the kernels are reused for each output\n",
    "- A convolutional layer can be seen as a discretized differential operator\n",
    "- Translation invariance, a desired physical property of the commutator\n",
    "  error we try to predict, is baked in.\n",
    "\n",
    "However, it only uses local spatial information, whereas an ideal closure\n",
    "model could maybe recover some of the missing information in far-away values.\n",
    "\n",
    "Note that we start by adding input channels, stored in a tuple of functions.\n",
    "The Burgers RHS has a square term, so maybe the closure model can make use of\n",
    "the same \"structure\" [^4]."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    create_cnn(; r, channels, σ, use_bias, rng, input_channels = (u -> u, u -> u .^ 2))\n",
    "\n",
    "Create CNN.\n",
    "\n",
    "Keyword arguments:\n",
    "\n",
    "- `r`: Vector of kernel radii\n",
    "- `channels`: Vector layer output channel numbers\n",
    "- `σ`: Vector of activation functions\n",
    "- `use_bias`: Vectors of indicators for using bias\n",
    "- `rng`: Random number generator\n",
    "- `input_channels`: Tuple of input channel contstructors\n",
    "\n",
    "Return `(cnn, θ)`, where `cnn(v, θ)` acts like a force on `v`.\n",
    "\"\"\"\n",
    "function create_cnn(; r, channels, σ, use_bias, rng, input_channels = (u -> u, u -> u .^ 2))\n",
    "    @assert channels[end] == 1 \"A unique output channel is required\"\n",
    "\n",
    "    # Add number of input channels\n",
    "    channels = [length(input_channels); channels]\n",
    "\n",
    "    # Padding length\n",
    "    padding = sum(r)\n",
    "\n",
    "    # Create CNN\n",
    "    create_model(\n",
    "        Chain(\n",
    "            # Create singleton channel\n",
    "            u -> reshape(u, size(u, 1), 1, size(u, 2)),\n",
    "\n",
    "            # Create input channels\n",
    "            u -> hcat(map(i -> i(u), input_channels)...),\n",
    "\n",
    "            # Add padding so that output has same shape as commutator error\n",
    "            u -> pad_circular(u, padding; dims = 1),\n",
    "\n",
    "            # Some convolutional layers\n",
    "            (\n",
    "                Conv(\n",
    "                    (2 * r[i] + 1,),\n",
    "                    channels[i] => channels[i+1],\n",
    "                    σ[i];\n",
    "                    use_bias = use_bias[i],\n",
    "                    init_weight = glorot_uniform_64,\n",
    "                ) for i ∈ eachindex(r)\n",
    "            )...,\n",
    "\n",
    "            # Remove singleton output channel\n",
    "            u -> reshape(u, size(u, 1), size(u, 3)),\n",
    "        ),\n",
    "        rng,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fourier neural operator architecture (FNO)\n",
    "\n",
    "Let's implement the FNO [^3]. It is a network composed of _Fourier Layers_\n",
    "(FL). A Fourier layer $u \\mapsto w$ transforms the _function_ $u$ into the\n",
    "_function_ $w$. It is defined by the following expression in continuous\n",
    "physical space:\n",
    "\n",
    "$$\n",
    "w(x) = \\sigma \\left( z(x) + W u(x) \\right), \\quad \\forall x \\in \\Omega,\n",
    "$$\n",
    "\n",
    "where $z$ is defined by its Fourier series coefficients $\\hat{z}(k) = R(k)\n",
    "\\hat{u}(k)$ for all wave numbers $k \\in \\mathbb{Z}$ and some weight matrix\n",
    "collection $R(k) \\in \\mathbb{C}^{n_\\text{out} \\times n_\\text{in}}$. The\n",
    "important part is the following choice: $R(k) = 0$ for $\\| k \\| >\n",
    "k_\\text{max}$ for some $k_\\text{max}$. This truncation makes the FNO\n",
    "applicable to any spatial $N$-discretization of $u$ and $w$ as long as $N > 2\n",
    "k_\\text{max}$. The same weight matrices may be reused for different\n",
    "discretizations.\n",
    "\n",
    "Note that a standard convolutional layer (CL) can also be written in spectral\n",
    "space, where the spatial convolution operation becomes an wave number\n",
    "element-wise product. The effective difference between the layers of a FNO\n",
    "and CNN becomes the following:\n",
    "\n",
    "- A FL does not include the bias of a CL\n",
    "- The spatial part of a FL corresponds to the central weight of the CL\n",
    "  kernel\n",
    "- A CL is chosen to be sparse in physical space (local kernel with radius $r\n",
    "  \\ll N / 2$), and would therefore be dense in spectral space ($k_\\text{max} =\n",
    "  N / 2$)\n",
    "- The spectral part of a FL is chosen to be sparse in spectral space\n",
    "  ($k_\\text{max} \\ll N / 2$), and would therefore dense in physical space (it\n",
    "  can be written as a convolution stencil with radius $r = N / 2$)\n",
    "\n",
    "Lux lets us define\n",
    "our own layer types. All functions should be \"pure\" (functional programming),\n",
    "meaning that the same inputs should produce the same outputs. In particular,\n",
    "this also applies to random number generation and state modification. The\n",
    "weights are stored in a vector outside the layer, while the layer itself\n",
    "contains information for weight initialization.\n",
    "\n",
    "If you are not familiar with Julia, feel free to go quickly through\n",
    "the following code cells. Just note that all variables have a type (e.g.\n",
    "`kmax::Int` means that `kmax` is an integer), but most of the time we don't have\n",
    "to declare types explicitly. Structures (`struct`s) can be\n",
    "parametrized and specialized for the types we give them in the constructor\n",
    "(e.g. `σ::A` means that a specialized version of the struct is compiled for\n",
    "each activation function we give it, creating an optimized FourierLayer for\n",
    "`σ = relu` where `A = typeof(relu)`, and a different version optimized for `σ\n",
    "= tanh` where `A = typeof(tanh)` etc.). Here our layer will have the type\n",
    "`FourierLayer`, with a default and custom constructor (two constructor\n",
    "methods, the latter making use of the default)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct FourierLayer{A,F} <: Lux.AbstractExplicitLayer\n",
    "    kmax::Int\n",
    "    cin::Int\n",
    "    cout::Int\n",
    "    σ::A\n",
    "    init_weight::F\n",
    "end\n",
    "\n",
    "FourierLayer(kmax, ch::Pair{Int,Int}; σ = identity, init_weight = glorot_uniform_64) =\n",
    "    FourierLayer(kmax, first(ch), last(ch), σ, init_weight)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also need to specify how to initialize the parameters and states. The\n",
    "Fourier layer does not have any hidden states that are modified. The below\n",
    "code adds methods to some existing Lux functions. These new methods are only\n",
    "used when the functions encounter `FourierLayer` inputs. For example, in our\n",
    "current environment, we have this many methods for the function\n",
    "`Lux.initialparameters` (including `Dense`, `Conv`, etc.):"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "length(methods(Lux.initialparameters))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we add our own method, there should be one more in the method\n",
    "table."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Lux.initialparameters(rng::AbstractRNG, (; kmax, cin, cout, init_weight)::FourierLayer) = (;\n",
    "    spatial_weight = init_weight(rng, cout, cin),\n",
    "    spectral_weights = init_weight(rng, kmax + 1, cout, cin, 2),\n",
    ")\n",
    "Lux.initialstates(::AbstractRNG, ::FourierLayer) = (;)\n",
    "Lux.parameterlength((; kmax, cin, cout)::FourierLayer) =\n",
    "    cout * cin + (kmax + 1) * 2 * cout * cin\n",
    "Lux.statelength(::FourierLayer) = 0\n",
    "\n",
    "# Pretty printing\n",
    "function Base.show(io::IO, (; kmax, cin, cout, σ)::FourierLayer)\n",
    "    print(io, \"FourierLayer(\", kmax)\n",
    "    print(io, \", \", cin, \" => \", cout)\n",
    "    print(io, \"; σ = \", σ)\n",
    "    print(io, \")\")\n",
    "end\n",
    "\n",
    "# One more method now\n",
    "length(methods(Lux.initialparameters))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is one of the advantages of Julia: As users we can extend functions from\n",
    "other authors without modifying their package or being forced to \"inherit\"\n",
    "their data structures (classes). This has created an interesting package\n",
    "ecosystem. For example, [ODE\n",
    "solvers](https://github.com/SciML/OrdinaryDiffEq.jl) can be used with exotic\n",
    "number types such as `BigFloat`, dual numbers, or quaternions. [Iterative\n",
    "solvers](https://github.com/JuliaLinearAlgebra/IterativeSolvers.jl)\n",
    "work out of the box with different array types, including various [GPU\n",
    "arrays](https://github.com/JuliaGPU/GPUArrays.jl), without actually\n",
    "containing any GPU array specific code.\n",
    "\n",
    "We now define how to pass inputs through a Fourier layer. In tensor notation,\n",
    "multiple samples can be processed at the same time. We therefore assume the\n",
    "following:\n",
    "\n",
    "- Input size: `(nx, cin, nsample)`\n",
    "- Output size: `(nx, cout, nsample)`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# This makes FourierLayers callable\n",
    "function ((; kmax, cout, cin, σ)::FourierLayer)(x, params, state)\n",
    "    nx = size(x, 1)\n",
    "\n",
    "    # Destructure params\n",
    "    # The real and imaginary parts of R are stored in two separate channels\n",
    "    W = params.spatial_weight\n",
    "    W = reshape(W, 1, cout, cin)\n",
    "    R = params.spectral_weights\n",
    "    R = selectdim(R, 4, 1) .+ im .* selectdim(R, 4, 2)\n",
    "\n",
    "    # Spatial part (applied point-wise)\n",
    "    y = reshape(x, nx, 1, cin, :)\n",
    "    y = sum(W .* y; dims = 3)\n",
    "    y = reshape(y, nx, cout, :)\n",
    "\n",
    "    # Spectral part (applied mode-wise)\n",
    "    #\n",
    "    # Steps:\n",
    "    #\n",
    "    # - go to complex-valued spectral space\n",
    "    # - chop off high wavenumbers\n",
    "    # - multiply with weights mode-wise\n",
    "    # - pad with zeros to restore original shape\n",
    "    # - go back to real valued spatial representation\n",
    "    ikeep = 1:kmax+1\n",
    "    nkeep = kmax + 1\n",
    "    z = fft(x, 1)\n",
    "    z = z[ikeep, :, :]\n",
    "    z = reshape(z, nkeep, 1, cin, :)\n",
    "    z = sum(R .* z; dims = 3)\n",
    "    z = reshape(z, nkeep, cout, :)\n",
    "    z = vcat(z, zeros(nx - kmax - 1, size(z, 2), size(z, 3)))\n",
    "    z = real.(ifft(z, 1))\n",
    "\n",
    "    # Outer layer: Activation over combined spatial and spectral parts\n",
    "    # Note: Even though high wavenumbers are chopped off in `z` and may\n",
    "    # possibly not be present in the input at all, `σ` creates new high\n",
    "    # wavenumbers. High wavenumber functions may thus be represented using a\n",
    "    # sequence of Fourier layers. In this case, the `y`s are the only place\n",
    "    # where information contained in high input wavenumbers survive in a\n",
    "    # Fourier layer.\n",
    "    w = σ.(z .+ y)\n",
    "\n",
    "    # Fourier layer does not modify state\n",
    "    w, state\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will chain some Fourier layers, with a final dense layer. As for the CNN,\n",
    "we allow for a tuple of predetermined input channels."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "    create_fno(; channels, kmax, σ, rng, input_channels = (u -> u, u -> u .^ 2))\n",
    "\n",
    "Create FNO.\n",
    "\n",
    "Keyword arguments:\n",
    "\n",
    "- `channels`: Vector of output channel numbers\n",
    "- `kmax`: Vector of cut-off wavenumbers\n",
    "- `σ`: Vector of activation functions\n",
    "- `rng`: Random number generator\n",
    "- `input_channels`: Tuple of input channel constructors\n",
    "\n",
    "Return `(fno, θ)`, where `fno(v, θ)` acts like a force on `v`.\n",
    "\"\"\"\n",
    "function create_fno(; channels, kmax, σ, rng, input_channels = (u -> u, u -> u .^ 2))\n",
    "    # Add number of input channels\n",
    "    channels = [length(input_channels); channels]\n",
    "\n",
    "    # Model\n",
    "    create_model(\n",
    "        Chain(\n",
    "            # Create singleton channel\n",
    "            u -> reshape(u, size(u, 1), 1, size(u, 2)),\n",
    "\n",
    "            # Create input channels\n",
    "            u -> hcat(map(i -> i(u), input_channels)...),\n",
    "\n",
    "            # Some Fourier layers\n",
    "            (\n",
    "                FourierLayer(kmax[i], channels[i] => channels[i+1]; σ = σ[i]) for\n",
    "                i ∈ eachindex(kmax)\n",
    "            )...,\n",
    "\n",
    "            # Put channels in first dimension\n",
    "            u -> permutedims(u, (2, 1, 3)),\n",
    "\n",
    "            # Compress with a final dense layer\n",
    "            Dense(channels[end] => 2 * channels[end], gelu),\n",
    "            Dense(2 * channels[end] => 1; use_bias = false),\n",
    "\n",
    "            # Put channels back after spatial dimension\n",
    "            u -> permutedims(u, (2, 1, 3)),\n",
    "\n",
    "            # Remove singleton channel\n",
    "            u -> reshape(u, size(u, 1), size(u, 3)),\n",
    "        ),\n",
    "        rng,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting to business: Training and comparing closure models\n",
    "\n",
    "If you have made it this far, you will probably have noticed that we have\n",
    "not yet done any computation (apart from one anecdotical Burgers simulation).\n",
    "We have only defined functions: losses, initializers, models, training...\n",
    "\n",
    "Now we set up an experiment. We need to decide on the following:\n",
    "\n",
    "- Problem parameter: $\\nu$\n",
    "- LES resolution\n",
    "- DNS resolution\n",
    "- Discrete filter\n",
    "- Number of initial conditions\n",
    "- Closure model: FNO and CNN\n",
    "- Simulation time: Too short, and we won't have time to detect instabilities\n",
    "  created by our model; too long, and most of the data will be too smooth for\n",
    "  a closure model to be needed (due to viscosity)\n",
    "\n",
    "In addition, we will split our data into\n",
    "\n",
    "- Training data (for choosing $\\theta$)\n",
    "- Validation data (just for monitoring training, choose when to stop)\n",
    "- Testing data (for testing performance on unseen data)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discretization and filter\n",
    "\n",
    "We will use a Gaussian filter kernel, truncated to zero outside of $3 / 2$\n",
    "filter widths."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Resolution\n",
    "nx_les = 64\n",
    "nx_dns = 512\n",
    "\n",
    "# Grids\n",
    "x_les = LinRange(0.0, 1.0, nx_les + 1)[2:end]\n",
    "x_dns = LinRange(0.0, 1.0, nx_dns + 1)[2:end]\n",
    "\n",
    "# Grid sizes\n",
    "Δx_les = 1 / nx_les\n",
    "Δx_dns = 1 / nx_dns\n",
    "\n",
    "# Filter width\n",
    "Δϕ = 3 * Δx_les\n",
    "\n",
    "# Filter kernel\n",
    "gaussian(Δ, x) = sqrt(6 / π) / Δ * exp(-6x^2 / Δ^2)\n",
    "top_hat(Δ, x) = (abs(x) ≤ Δ / 2) / Δ\n",
    "kernel = gaussian\n",
    "\n",
    "# Discrete filter matrix (with periodic extension and threshold for sparsity)\n",
    "ϕ = sum(-1:1) do z\n",
    "    d = @. x_les - x_dns' - z\n",
    "    @. kernel(Δϕ, d) * (abs(d) ≤ 3 / 2 * Δϕ)\n",
    "end\n",
    "ϕ = ϕ ./ sum(ϕ; dims = 2) ## Normalize weights\n",
    "ϕ = sparse(ϕ)\n",
    "dropzeros!(ϕ)\n",
    "heatmap(ϕ; yflip = true, xmirror = true, title = \"Filter matrix\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create data\n",
    "\n",
    "Create the training, validation, and testing datasets.\n",
    "Use a different time step for testing to detect overfitting."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ν = 1.0e-3\n",
    "data_train = create_data(10, ϕ; nt = 1000, dt = 1.0e-3, ν)\n",
    "data_valid = create_data(2, ϕ; nt = 100, dt = 1.1e-3, ν)\n",
    "data_test = create_data(5, ϕ; nt = 1000, dt = 0.8e-3, ν)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Closure models\n",
    "\n",
    "We also include a \"no closure\" model (baseline for comparison)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "noclosure, θ_noclosure = (u, θ) -> zero(u), nothing"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train a CNN\n",
    "\n",
    "Create CNN model. Note that the last activation is `identity`, as we don't\n",
    "want to restrict the output values. We can inspect the structure in the\n",
    "wrapped Lux `Chain`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cnn, θ_cnn = create_cnn(;\n",
    "    r = [2, 2, 2, 2],\n",
    "    channels = [8, 8, 8, 1],\n",
    "    σ = [leakyrelu, leakyrelu, leakyrelu, identity],\n",
    "    use_bias = [true, true, true, false],\n",
    "    rng,\n",
    ")\n",
    "cnn.chain"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = create_randloss_commutator(cnn, data_train; nuse = 50)\n",
    "# loss = create_randloss_trajectory(g, data_train; nuse = 3, nunroll = 10, ν, m = cnn)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initilize CNN training state"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trainstate = initial_trainstate(Adam(1.0e-3), θ_cnn)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model warm-up: trigger compilation and get indication of complexity"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss(θ_cnn)\n",
    "gradient(loss, θ_cnn);\n",
    "@time loss(θ_cnn);\n",
    "@time gradient(loss, θ_cnn);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the CNN. The cell below can be repeated to continue training where the\n",
    "previous training session left off."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trainstate = train(;\n",
    "    trainstate...,\n",
    "    loss,\n",
    "    niter = 2000,\n",
    "    ncallback = 20,\n",
    "    callback = create_callback(cnn, data_valid),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final CNN weights"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "θ_cnn = trainstate.θ"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train an FNO\n",
    "\n",
    "Create FNO. Like for the CNN, last activation is `identity`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fno, θ_fno = create_fno(;\n",
    "    channels = [5, 5, 5, 5],\n",
    "    kmax = [16, 16, 16, 8],\n",
    "    σ = [gelu, gelu, gelu, identity],\n",
    "    rng,\n",
    ")\n",
    "fno.chain"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose loss function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = create_randloss_commutator(fno, data_train; nuse = 50)\n",
    "# loss = create_randloss_trajectory(g, data_train; nuse = 3, nunroll = 10, ν, m = fno)\n",
    "\n",
    "trainstate = initial_trainstate(Adam(1.0e-3), θ_fno)\n",
    "\n",
    "# Model warm-up: trigger compilation and get indication of complexity\n",
    "loss(θ_fno);\n",
    "gradient(loss, θ_fno);\n",
    "@time loss(θ_fno);\n",
    "@time gradient(loss, θ_fno);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the FNO. The cell below can be repeated to continue training where the\n",
    "previous training session left off."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "trainstate = train(;\n",
    "    trainstate...,\n",
    "    loss,\n",
    "    niter = 1000,\n",
    "    ncallback = 20,\n",
    "    callback = create_callback(fno, data_valid),\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final FNO weights"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "θ_fno = trainstate.θ"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model performance\n",
    "\n",
    "We will now make a comparison of the three closure models (including the\n",
    "\"no-model\" where $m = 0$, which corresponds to solving the DNS equations on the\n",
    "LES grid)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = [noclosure, cnn, fno]\n",
    "θ = [θ_noclosure, θ_cnn, θ_fno]\n",
    "labels = [\"0\", \"CNN\", \"FNO\"]\n",
    "\n",
    "println(\"Relative a posteriori errors:\")\n",
    "for i in eachindex(m)\n",
    "    e = trajectory_error(g, data_test.u; data_test.dt, ν, m = m[i], θ = θ[i])\n",
    "    print(labels[i])\n",
    "    print(\":\\t \")\n",
    "    print(e)\n",
    "    println()\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling exercices\n",
    "\n",
    "To get confident with modeling ODE right hand sides using machine learning,\n",
    "it can be useful to do one or more of the following exercices.\n",
    "\n",
    "### Trajectory fitting (a posteriori loss function)\n",
    "\n",
    "- Fit a closure model using the a posteriori loss function.\n",
    "- Investigate the effect of the parameter `nunroll`. Try for example `@time\n",
    "  randloss(θ)` for `unroll = 10` and `nunroll = 20` (execute `randloss` once\n",
    "  first to trigger compilation).\n",
    "- Discuss the statement \"$L^\\text{prior}$ and $L^\\text{post}$ are almost the\n",
    "  same when `nunroll = 1`\" with your neighbour. Are they exactly the same if\n",
    "  we use forward Euler ($u^{n + 1} = u^n + \\Delta t f(u^n)$) instead of RK4?\n",
    "\n",
    "### Neural ODE (brute force right hand side)\n",
    "\n",
    "1. Observe that, if we really want to, we can skip the term $f(\\bar{u})$\n",
    "   entirely, hoping that $m$ will be able to model it directly (in addition\n",
    "   to the commutator error). The resulting model is\n",
    "   $$\n",
    "   \\frac{\\mathrm{d} \\bar{v}}{\\mathrm{d} t} = m(\\bar{v}, \\theta).\n",
    "   $$\n",
    "   This is known as a _Neural ODE_ [^5].\n",
    "1. Rewrite the function `g` such that the closure model predicts the _entire_\n",
    "   right hand side instead of the correction only. (Comment out `f` in `g`).\n",
    "1. Train the CNN or FNO in this setting. Is the model able to represent the\n",
    "   solution correctly?\n",
    "\n",
    "### Learn the discretization\n",
    "\n",
    "- Make a new instance of the CNN closure, called `cnn_linear` with parameters\n",
    "  `θ_cnn_linear`, which only has one convolutional layer.\n",
    "  This model should still add the square input channel.\n",
    "- Observe that the original Burgers DNS RHS $f$ can actually be expressed in\n",
    "  its entirety using this model, i.e.\n",
    "  $$\n",
    "  \\frac{\\mathrm{d} u}{\\mathrm{d} t} = f(u) = \\operatorname{CNN}(u,\n",
    "  \\theta).\n",
    "  $$\n",
    "  - What is the kernel radius?\n",
    "  - Should there still be a nonlinear activation function?\n",
    "  - What is the exact expression for the model weights and bias?\n",
    "- \"Improve\" the discretization $f$: Increase the kernel radius of\n",
    "  `cnn_linear` and train the model. What does the resulting kernel stencil\n",
    "  (`Array(θ_cnn_linear)`) look like? Does it resemble the one of $f$?\n",
    "\n",
    "### Naive neural closure model\n",
    "\n",
    "Create a new instance of the CNN or FNO models, called `cnn_naive` or\n",
    "`fno_naive`, without the additional square input channel (prior physical\n",
    "knowledge). The input should only have one singleton channel (pass the\n",
    "keyword argument `input_channels = (u -> u,)` to the constructor).\n",
    "Do you expect this version to perform better or worse than with a square\n",
    "channel?\n",
    "\n",
    "## References\n",
    "\n",
    "[^3]: Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A.\n",
    "      Stuart, and A. Anandkumar.\n",
    "      _Fourier neural operator for parametric partial differential\n",
    "      equations._\n",
    "      arXiv:[2010.08895](https://arxiv.org/abs/2010.08895), 2021.\n",
    "\n",
    "[^4]: Hugo Melchers, Daan Crommelin, Barry Koren, Vlado Menkovski, Benjamin\n",
    "      Sanderse,\n",
    "      _Comparison of neural closure models for discretised PDEs_,\n",
    "      Computers & Mathematics with Applications,\n",
    "      Volume 143,\n",
    "      2023,\n",
    "      Pages 94-107,\n",
    "      ISSN 0898-1221,\n",
    "      <https://doi.org/10.1016/j.camwa.2023.04.030>.\n",
    "\n",
    "[^5]: R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud.\n",
    "      _Neural Ordinary Differential Equations_.\n",
    "      arXiv:[1806.07366](https://arxiv.org/abs/1806.07366), 2018."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
